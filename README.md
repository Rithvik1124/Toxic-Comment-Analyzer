# Toxic-Comment-Analyzer
ðŸ’€ðŸ‘»
A logistic regression trained model that checks whether the inputted comment is toxic or not

The application provides both a graphical user interface (GUI) and functionalities for analyzing and visualizing datasets, including word clouds and distribution plots. It uses the dataset from the Toxic Comment Classification Challenge on Kaggle.

**Data Cleaning:**
 - Removes noise from text data, including punctuation, numeric tokens, non-ASCII characters, and newline characters.

**Data Visualization:**
 - Generates distribution plots for toxic comment categories.
 - Creates word clouds for specific categories like toxic or identity_hate.

**Machine Learning Model:**
 - Trains a logistic regression model to classify comments as toxic or not toxic.
 - Saves and loads models for reusability.

**Toxicity Prediction:**
 - Predicts whether a given comment is toxic through an easy-to-use GUI.

**Download Dataset:** Download the dataset from **"Toxic Comment Classification Challenge"** in **Kaggle**(https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data?select=train.csv.zip). Save the train.csv file in the project directory.

**Required Libraries:**  
   - `matplotlib`
   - `numpy`
   - `pandas`
   - `seaborn`
   - `scikit-learn`
   - `wordcloud`
   - `tkinter`
   - `nltk`

Contributions are welcomed!ðŸ˜Š

